{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f5d923",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "import hashlib\n",
    "import os\n",
    "from requests.exceptions import RequestException\n",
    "import logging\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "e47aca97",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_DIR = r\"C:/Users/HP/Documents/repos/news_ingestion_data_pipeline/data\"\n",
    "DB_FILE = \"articles.db\"\n",
    "DB_PATH = os.path.join(DB_DIR, DB_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d998db7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_news_time():\n",
    "    with sqlite3.connect(DB_PATH) as conn:\n",
    "        cursor = conn.execute(\"SELECT MAX(News_published_time) FROM articles\")\n",
    "        result = cursor.fetchone()\n",
    "        if result[0]:\n",
    "            incremental = True\n",
    "            latest_timestamp = result[0]\n",
    "        else:\n",
    "            incremental = False\n",
    "            latest_timestamp = '2025-07-25T00:00:00'\n",
    "\n",
    "        latest_timestamp = datetime.fromisoformat(latest_timestamp).replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "        return (latest_timestamp, incremental)\n",
    "    \n",
    "def upsert_articles(filtered_articles):\n",
    "    with sqlite3.connect(DB_PATH) as conn:\n",
    "        for article in filtered_articles:\n",
    "            sql = \"\"\"\n",
    "            INSERT INTO articles (Article_id,  News_link, News_title, Author_name, News_published_time, Source_name, Processed_at)\n",
    "            VALUES (?, ?,  ?, ?, ?, ?, CURRENT_TIMESTAMP)\n",
    "            ON CONFLICT(Article_id) DO UPDATE SET\n",
    "                News_link = excluded.News_link,\n",
    "                News_title = excluded.News_title,\n",
    "                Author_name = excluded.Author_name,\n",
    "                News_published_time = excluded.News_published_time,\n",
    "                Source_name = excluded.Source_name,\n",
    "                Processed_at = CURRENT_TIMESTAMP\n",
    "            \"\"\"\n",
    "            params = (\n",
    "                article.get('Article_id'),\n",
    "                article.get('News_link'),\n",
    "                article.get('News_title'),\n",
    "                article.get('Author_name'),\n",
    "                article.get('News_published_time'),\n",
    "                article.get('Source_name')\n",
    "            )\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(sql, params)\n",
    "\n",
    "def query_topn_articles(n = 5):\n",
    "    # Connect to the SQLite database\n",
    "    with sqlite3.connect(DB_PATH) as conn:\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Execute a query to select all articles\n",
    "        cursor.execute(f\"\"\"SELECT Article_id, News_link, News_title,Author_name, News_published_time, Source_name, Processed_at FROM articles\n",
    "                        ORDER BY News_published_time DESC LIMIT {n}\"\"\")\n",
    "\n",
    "        # Fetch all rows returned by the query\n",
    "        rows = cursor.fetchall()\n",
    "        # Process and display results\n",
    "        for row in rows:\n",
    "            print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "31c92a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_date(date_str, source= 'Skift'):\n",
    "    \"\"\"\n",
    "    Converts date string like 'July 28, 2025' to datetime object.\n",
    "    Return None if parse fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if source == \"Phocuswire\":\n",
    "            return datetime.strptime(date_str.strip(), \"%B %d, %Y\")\n",
    "        else:\n",
    "            return datetime.fromisoformat(date_str)\n",
    "    except Exception:\n",
    "        return None\n",
    "    \n",
    "def drop_timezone(date_str):\n",
    "    date_str = datetime.fromisoformat(date_str)\n",
    "    return date_str.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "    \n",
    "def generate_article_id(url):\n",
    "    return hashlib.md5(url.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "\n",
    "def datetime_to_iso_with_time(dt):\n",
    "    \"\"\"\n",
    "    Convert a datetime object to ISO8601 string with a fixed time part.\n",
    "\n",
    "    Args:\n",
    "        dt (datetime): A datetime object (date part used).\n",
    "\n",
    "    Returns:\n",
    "        str: Formatted ISO8601 string in 'YYYY-MM-DDTHH:MM:SS' format\n",
    "    \"\"\"\n",
    "    date_part = dt.strftime(\"%Y-%m-%d\")\n",
    "    time_str = dt.strftime(\"%H:%M:%S\")\n",
    "    return f\"{date_part}T{time_str}\"\n",
    "\n",
    "def fetch_url_with_retries(url, headers = {}, max_retries=3, backoff_factor=1.0, timeout=10):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            if headers: \n",
    "                response = requests.get(url, timeout=timeout, headers=headers)\n",
    "            else:\n",
    "                response = requests.get(url, timeout=timeout)\n",
    "            response.raise_for_status()\n",
    "            return response\n",
    "        except RequestException as e:\n",
    "            wait = backoff_factor * (2 ** attempt)\n",
    "            print(f\"Request failed: {e}. Retrying in {wait:.1f} seconds (Attempt {attempt + 1} of {max_retries})\")\n",
    "            time.sleep(wait)\n",
    "    print(f\"Failed to fetch {url} after {max_retries} attempts.\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "8f18699f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkiftScraper:\n",
    "    def __init__(self, min_delay=1, max_delay=3, max_pages=15, max_retries = 3, backoff_factor= 1.0,timeout = 10 ):\n",
    "        \"\"\"\n",
    "        Initialize the SkiftScraper.\n",
    "        :param last_ingested_date: datetime.datetime or None, stop scraping older news\n",
    "        :param min_delay: minimum delay between requests (seconds)\n",
    "        :param max_delay: maximum delay between requests (seconds)\n",
    "        :param max_pages: max number of pages to scrape\n",
    "        \"\"\"\n",
    "        self.base_url = \"https://skift.com/news/\"\n",
    "        self.headers = {}\n",
    "        self.sourcename = 'Skift'\n",
    "        self.min_delay = min_delay\n",
    "        self.max_delay = max_delay\n",
    "        self.max_pages = max_pages\n",
    "        self.max_retries = max_retries\n",
    "        self.backoff_factor = backoff_factor\n",
    "        self.timeout = timeout\n",
    "        self.collected_articles = []\n",
    "        self.seen_article_ids = set()\n",
    "\n",
    "    def fetch_url_with_retries(self, url):\n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                if self.headers: \n",
    "                    response = requests.get(url, timeout=self.timeout, headers=self.headers)\n",
    "                else:\n",
    "                    response = requests.get(url, timeout=self.timeout)\n",
    "                response.raise_for_status()\n",
    "                return response\n",
    "            except RequestException as e:\n",
    "                wait = self.backoff_factor * (2 ** attempt)\n",
    "                print(f\"Request failed: {e}. Retrying in {wait:.1f} seconds (Attempt {attempt + 1} of {self.max_retries})\")\n",
    "                time.sleep(wait)\n",
    "        print(f\"Failed to fetch {url} after {self.max_retries} attempts.\")\n",
    "        return None\n",
    "\n",
    "    def get_page_url(self, page):\n",
    "        return f\"{self.base_url}page/{page}/\"\n",
    "\n",
    "    def extract_articles(self,last_ingested_date):\n",
    "        page = 1\n",
    "\n",
    "        while True:\n",
    "            url = self.get_page_url(page)\n",
    "            response = self.fetch_url_with_retries(url)\n",
    "\n",
    "            if not response:\n",
    "                print(f\"Stopping scraping due to repeated request failures at page {page}\")\n",
    "                break\n",
    "\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            articles = soup.select(\"article\")\n",
    "\n",
    "            if not articles:\n",
    "                print(f\"No articles found on page {page}, stopping.\")\n",
    "                break\n",
    "\n",
    "            stop_paging = False\n",
    "\n",
    "            for article in articles:\n",
    "                link_tag = article.select_one(\"h3.c-tease__title a\")\n",
    "                if not link_tag:\n",
    "                    print(\"Article missing title link, skipping.\")\n",
    "                    continue\n",
    "\n",
    "                news_url = link_tag.get('href')\n",
    "                if not news_url:\n",
    "                    print(\"Article missing href link, skipping.\")\n",
    "                    continue\n",
    "\n",
    "                article_id = generate_article_id(news_url)\n",
    "                if article_id in self.seen_article_ids:\n",
    "                    print(f\"Duplicate article {article_id} found, skipping.\")\n",
    "                    continue\n",
    "                \n",
    "                headline = link_tag.text.strip() if link_tag else None\n",
    "                author_tag = article.select_one(\"div.c-tease__byline a.underline\")\n",
    "                if not author_tag:\n",
    "                    print(f\"Author not available for article id {article_id} and article headline {headline}.\")\n",
    "                author_name = author_tag.text.strip() if author_tag else None\n",
    "\n",
    "                time_tag = article.select_one(\"div.c-tease__byline time\")            \n",
    "                news_time = drop_timezone(time_tag.get(\"datetime\")) if time_tag else None\n",
    "                \n",
    "                try:\n",
    "                    news_time = parse_date(news_time)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error parsing date '{news_time}': {e}\")\n",
    "                    news_time = None\n",
    "                if news_time :\n",
    "                    if last_ingested_date and news_time < last_ingested_date:\n",
    "                        # If last_ingested_date is set and this article is older or equal, stop ingestion\n",
    "                        stop_paging = True\n",
    "                        print(f\"Encountered article dated {news_time} < last ingested {last_ingested_date}, stopping.\")\n",
    "                        break\n",
    "                else:\n",
    "                    # If no date found, you can decide to skip or include\n",
    "                    print(\"Article without date found, skipping date check.\")\n",
    "                \n",
    "\n",
    "                article_data = {\n",
    "                    \"Article_id\": article_id,\n",
    "                    \"News_title\": headline,\n",
    "                    \"News_link\": news_url,\n",
    "                    \"Author_name\": author_name,\n",
    "                    \"News_published_time\": datetime_to_iso_with_time(news_time),\n",
    "                    \"Source_name\": \"Skift\"\n",
    "                }\n",
    "                self.collected_articles.append(article_data)\n",
    "                self.seen_article_ids.add(article_id)\n",
    "                \n",
    "            if stop_paging:\n",
    "                break\n",
    "\n",
    "            page += 1\n",
    "            delay = random.uniform(self.min_delay, self.max_delay)\n",
    "            print(f\"Sleeping for {delay:.1f} seconds before next page request.\")\n",
    "            time.sleep(delay)\n",
    "\n",
    "        print(f\"Total new articles extracted: {len(self.collected_articles)}\")\n",
    "        return self.collected_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "81320557",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PhocuswireScraper:\n",
    "    def __init__(self, min_delay=1, max_delay=3, max_pages=15, max_retries = 3, backoff_factor= 1.0,timeout = 10 ):\n",
    "        \"\"\"\n",
    "        Initialize the PhocusewireScraper.\n",
    "        :param min_delay: minimum delay between requests (seconds)\n",
    "        :param max_delay: maximum delay between requests (seconds)\n",
    "        :param max_pages: max number of pages to scrape\n",
    "        \"\"\"\n",
    "        self.base_url = \"https://www.phocuswire.com\"\n",
    "        self.headers =  {\n",
    "                        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '\n",
    "                                    'AppleWebKit/537.36 (KHTML, like Gecko) '\n",
    "                                    'Chrome/115.0.0.0 Safari/537.36'\n",
    "                        }\n",
    "        self.min_delay = min_delay\n",
    "        self.source_name = \"Phocuswire\"\n",
    "        self.max_delay = max_delay\n",
    "        self.max_pages = max_pages\n",
    "        self.max_retries = max_retries\n",
    "        self.backoff_factor = backoff_factor\n",
    "        self.timeout = timeout\n",
    "        self.collected_articles = []\n",
    "        self.seen_article_ids = set()\n",
    "\n",
    "    def fetch_url_with_retries(self, url):\n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                if self.headers: \n",
    "                    response = requests.get(url, timeout=self.timeout, headers=self.headers)\n",
    "                else:\n",
    "                    response = requests.get(url, timeout=self.timeout)\n",
    "                response.raise_for_status()\n",
    "                return response\n",
    "            except RequestException as e:\n",
    "                wait = self.backoff_factor * (2 ** attempt)\n",
    "                print(f\"Request failed: {e}. Retrying in {wait:.1f} seconds (Attempt {attempt + 1} of {self.max_retries})\")\n",
    "                time.sleep(wait)\n",
    "        print(f\"Failed to fetch {url} after {self.max_retries} attempts.\")\n",
    "        return None\n",
    "\n",
    "    def get_page_url(self, page):\n",
    "        return f\"{self.base_url}/Latest-News?pg={page}\"\n",
    "\n",
    "    def extract_articles(self,last_ingested_date):\n",
    "        page = 1\n",
    "\n",
    "        while True:\n",
    "            url = self.get_page_url(page)\n",
    "            response = self.fetch_url_with_retries(url)\n",
    "\n",
    "            if not response:\n",
    "                print(f\"Stopping scraping due to repeated request failures at page {page}\")\n",
    "                break\n",
    "\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            articles = soup.select(\"div.article-list  div.item\")\n",
    "\n",
    "            if not articles:\n",
    "                print(f\"No articles found on page {page}, stopping.\")\n",
    "                break\n",
    "\n",
    "            stop_paging = False\n",
    "\n",
    "            for article in articles:\n",
    "                # Extract news date (inside div.author)\n",
    "                title_tag = article.select_one(\"a.title\")\n",
    "                if not title_tag:\n",
    "                    print(\"Article missing title link, skipping.\")\n",
    "                    continue\n",
    "\n",
    "                headline = title_tag.get_text(strip=True) if title_tag else None\n",
    "\n",
    "                if not title_tag['href']:\n",
    "                    print(\"Article missing href link, skipping.\")\n",
    "                    continue\n",
    "\n",
    "                news_url = f\"{self.base_url}/{title_tag['href']}\" if title_tag and 'href' in title_tag.attrs else None\n",
    "\n",
    "                article_id = generate_article_id(news_url) if news_url else None\n",
    "                if article_id in self.seen_article_ids:\n",
    "                    print(f\"Duplicate article {article_id} found, skipping.\")\n",
    "                    continue\n",
    "\n",
    "                author_span = article.select_one(\"div.author > span.name\")\n",
    "                if not author_span:\n",
    "                    print(f\"Author not available for article id {article_id} and article headline {headline}.\")\n",
    "                author_name = author_span.get_text(strip=True).replace(\"By \", \"\") if author_span else None\n",
    "                \n",
    "                # Extract news time from div.author text after the pipe symbol\n",
    "                author_div = article.select_one(\"div.author\")\n",
    "                news_time = None\n",
    "                if author_div:\n",
    "                    # The text looks like 'By Abby Crotty | July 28, 2025'\n",
    "                    # We can split by '|' and strip whitespace to get news time\n",
    "                    parts = author_div.text.split('|')\n",
    "                    if len(parts) == 2:\n",
    "                        news_time = parts[1].strip()\n",
    "                        \n",
    "                    try:\n",
    "                        news_time = parse_date(news_time, self.source_name)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error parsing date '{news_time}': {e}\")\n",
    "                        news_time = None\n",
    "                if news_time :\n",
    "                    if last_ingested_date and news_time < last_ingested_date:\n",
    "                        # If last_ingested_date is set and this article is older or equal, stop ingestion\n",
    "                        print(f\"News time : {news_time}\")\n",
    "                        print(f\"last -ngested time: {last_ingested_date}\")\n",
    "\n",
    "                        stop_paging = True\n",
    "                        print(f\"Encountered article dated {news_time} < last ingested {last_ingested_date}, stopping.\")\n",
    "                        break\n",
    "                else:\n",
    "                    # If no date found, you can decide to skip or include\n",
    "                    print(\"Article without date found, skipping date check.\")\n",
    "\n",
    "                \n",
    "                article_data = {\n",
    "                    \"Article_id\": article_id,\n",
    "                    \"News_title\": headline,\n",
    "                    \"News_link\": news_url,\n",
    "                    \"Author_name\": author_name,\n",
    "                    \"News_published_time\": datetime_to_iso_with_time(news_time),\n",
    "                    \"Source_name\": \"Phocuswire\"\n",
    "                }\n",
    "\n",
    "                self.collected_articles.append(article_data)\n",
    "                self.seen_article_ids.add(article_id)\n",
    "\n",
    "            if stop_paging:\n",
    "                break\n",
    "\n",
    "            page += 1\n",
    "            delay = random.uniform(self.min_delay, self.max_delay)\n",
    "            print(f\"Sleeping for {delay:.1f} seconds before next page request.\")\n",
    "            time.sleep(delay)\n",
    "\n",
    "        print(f\"Total new articles extracted: {len(self.collected_articles)}\")\n",
    "        return self.collected_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "498c25c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-01 00:00:00\n",
      "Initiating increment load...\n",
      "Latest record time stamp present in database :  2025-08-01 00:00:00\n",
      "Sleeping for 1.3 seconds before next page request.\n",
      "Encountered article dated 2025-07-31 17:48:19 < last ingested 2025-08-01 00:00:00, stopping.\n",
      "Total new articles extracted: 12\n",
      "News time : 2025-07-31 00:00:00\n",
      "last -ngested time: 2025-08-01 00:00:00\n",
      "Encountered article dated 2025-07-31 00:00:00 < last ingested 2025-08-01 00:00:00, stopping.\n",
      "Total new articles extracted: 3\n",
      "Total articles extracted :  15\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    latest_timestamp, is_incremental = get_latest_news_time()\n",
    "    print(f\"{latest_timestamp}\")\n",
    "    if is_incremental:\n",
    "        print(\"Initiating increment load...\")\n",
    "        print(\"Latest record time stamp present in database : \",latest_timestamp)\n",
    "    else:\n",
    "        print(\"Latest record timestamp not found in database.\")\n",
    "        print(\"Initiating full load...\")\n",
    "    extracted_articles = []\n",
    "    \n",
    "    Skriftscraper = SkiftScraper()\n",
    "    skift_articles = Skriftscraper.extract_articles(latest_timestamp)\n",
    "    extracted_articles.extend(skift_articles)\n",
    "    Phocuswirescraper = PhocuswireScraper()\n",
    "    phocuswire_articles = Phocuswirescraper.extract_articles(latest_timestamp)\n",
    "    extracted_articles.extend(phocuswire_articles)\n",
    "\n",
    "    print(\"Total articles extracted : \", len(extracted_articles))\n",
    "\n",
    "    upsert_articles(extracted_articles)\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "bad4f177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('b6180012cdfcaab01451bded2196d26c', 'https://skift.com/2025/08/01/from-concur-to-spotnana-steve-singh-on-how-ai-could-fix-corporate-travel/', 'From Concur to Spotnana: Steve Singh on How AI Could Fix Corporate Travel', \"Sean O'Neill\", '2025-08-01T17:19:21', 'Skift', '2025-08-02 21:18:26')\n",
      "('d030436466546bcf23aa4befbf4d08b6', 'https://skift.com/2025/08/01/delta-says-it-will-not-use-ai-to-target-customers/', 'Delta Responds to AI-Pricing Backlash: No ‘Individualized Prices Based on Personal Data’', 'Meghna Maharishi', '2025-08-01T15:05:14', 'Skift', '2025-08-02 21:18:26')\n",
      "('8db7ffa1ba14adc8dd8348e7f5d1923d', 'https://skift.com/2025/08/01/u-s-dollar-slide-hurts-accor-minor-and-melia/', 'U.S. Dollar Slide Hurts Accor, Minor, and Meliá', 'Luke Martin', '2025-08-01T13:53:29', 'Skift', '2025-08-02 21:18:26')\n",
      "('3fa30e2c6eee18976f224053633c1a27', 'https://skift.com/2025/08/01/winners-losers-and-lots-of-premium-seats-europes-airline-scorecard/', 'Winners, Losers, and Lots of Premium Seats: Europe’s Airline Scorecard', 'Gordon Smith', '2025-08-01T13:13:39', 'Skift', '2025-08-02 21:18:26')\n",
      "('265b2b2258dd197970b5619cbd6b943a', 'https://skift.com/2025/08/01/electrification-and-renewables-are-driving-iberostars-emissions-decline/', 'Electrification and Renewables Are Driving Iberostar’s Emissions Decline', 'Darin Graham', '2025-08-01T13:02:26', 'Skift', '2025-08-02 21:18:26')\n",
      "('faaffa0546e28c8a42886a98ae4eeb1f', 'https://skift.com/2025/08/01/unmanaged-business-travel-still-dominates-as-smbs-seek-flexible-consumer-style-booking/', 'Unmanaged Business Travel Still Dominates as SMBs Seek Flexible, Consumer-Style Booking', 'Clara Awuse', '2025-08-01T11:57:13', 'Skift', '2025-08-02 21:18:26')\n",
      "('b7b2ec4cf8809f9bf777cfccac1a3ef8', 'https://meetings.skift.com/2025/08/01/from-vegas-to-nashville-elon-musks-loop-targets-event-cities/', 'From Vegas to Nashville: Elon Musk’s Loop Targets Event Cities', 'Andrea Doyle', '2025-08-01T11:04:35', 'Skift', '2025-08-02 21:18:26')\n",
      "('0fe47c1e00042e927192adf61a4108e8', 'https://skift.com/2025/08/01/ihg-pursues-business-travelers-through-emirates-and-other-deals/', 'IHG Pursues Business Travelers Through Emirates and Other Deals', \"Sean O'Neill\", '2025-08-01T09:18:11', 'Skift', '2025-08-02 21:18:26')\n",
      "('6e3d2c63527467198251d568495dee15', 'https://skift.com/2025/08/01/chalet-hotels-names-shwetank-singh-ceo-as-sanjay-sethi-steps-down/', 'Chalet Hotels Names Shwetank Singh CEO As Sanjay Sethi Steps Down', 'Bulbul Dhawan', '2025-08-01T08:22:35', 'Skift', '2025-08-02 21:18:26')\n",
      "('cc4aa88bd65eb2f7db7b96aa58ed8414', 'https://meetings.skift.com/2025/08/01/9-ways-to-kickstart-your-career-as-a-special-event-planner/', '9 Ways to Kickstart Your Career as a Special Event Planner', 'Barbara Scofidio', '2025-08-01T08:18:00', 'Skift', '2025-08-02 21:18:26')\n",
      "('79f16f0413ea47dabf64485d2560ca54', 'https://skift.com/2025/08/01/luxury-travel-ai-personalization-manfredi-lefebvre/', 'Luxury Travel’s Next Era: AI, Empathy, and Expertise with Manfredi Lefebvre', 'Kanchi Jain', '2025-08-01T07:16:53', 'Skift', '2025-08-02 21:18:26')\n",
      "('e66f08ee027ccf783948ce4980ea4d19', 'https://skift.com/2025/08/01/booking-trends-ai-tools-and-the-new-rules-of-travel-planning/', 'Booking Trends, AI Tools, and the New Rules of Travel Planning', 'Rashaad Jorden', '2025-08-01T04:52:46', 'Skift', '2025-08-02 21:18:26')\n",
      "('57df0278655170f8de688fcd84bbd72b', 'https://www.phocuswire.com//penta-hotels-smart-host-phocuswright-europe-2025', 'The evolution of CRM in hotels', 'Linda Fox', '2025-08-01T00:00:00', 'Phocuswire', '2025-08-02 21:18:26')\n",
      "('6c2fe2b95e5b4febb64ba2105cf07582', 'https://www.phocuswire.com//phocuswright-europe-2025-bridge-series-next-gen-travel-products-services', 'Weighing in on the next generation of travel products and services around the world', 'Abby Crotty', '2025-08-01T00:00:00', 'Phocuswire', '2025-08-02 21:18:26')\n",
      "('7ac8bd31b72b6538309c54d643853517', 'https://www.phocuswire.com//travel-tech-news-briefs-aug-1-2025', \"PhocusWire's weekly travel tech news briefs: Casago, GuideGeek, Booking.com and more...\", 'PhocusWire', '2025-08-01T00:00:00', 'Phocuswire', '2025-08-02 21:18:26')\n",
      "('dbbd72a1d526c281efcf8b8c02415743', 'https://skift.com/2025/07/31/southwest-airlines-appoints-new-independent-chairman-in-latest-board-shakeup/', 'Southwest Airlines Appoints New Independent Chairman in Latest Board Shakeup', 'Meghna Maharishi', '2025-07-31T17:48:19', 'Skift', '2025-08-02 21:18:10')\n",
      "('8d54e449e391f34406a0d145563072b4', 'https://skift.com/2025/07/31/dont-count-out-google-travel-search-booking-com-still-sees-gains/', 'Don’t Count Out Google Travel Search –\\xa0Booking.com Still Sees Gains', 'Dennis Schaal', '2025-07-31T16:27:43', 'Skift', '2025-08-02 21:18:10')\n",
      "('8df2a315e93092ed041a033719d28c88', 'https://skift.com/2025/07/31/amadeus-and-google-join-forces-again-to-bolster-flights-business/', 'Amadeus and Google Join Forces — Again — To Bolster Flights Business', 'Dennis Schaal', '2025-07-31T13:05:53', 'Skift', '2025-08-02 21:18:10')\n"
     ]
    }
   ],
   "source": [
    "query_topn_articles(18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "456019f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(97,)\n"
     ]
    }
   ],
   "source": [
    "with sqlite3.connect(DB_PATH) as conn:\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Execute a query to select all articles\n",
    "    cursor.execute(\"\"\"SELECT Count(*) FROM articles\n",
    "                    \"\"\")\n",
    "\n",
    "    # Fetch all rows returned by the query\n",
    "    rows = cursor.fetchall()\n",
    "    # Process and display results\n",
    "    for row in rows:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea02b3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
