    # base_url = "https://skift.com/news/"
    # page = 1

    # articles_page_data = []
    # while page <= 15:
    #     # Compose the page URL (first page is just the base, subsequent pages are /page/2/, etc.)
    #     url = f"{base_url}page/{page}/"

    #     response = requests.get(url)
    #     if response.status_code != 200:
    #         break  # No more pages

    #     soup = BeautifulSoup(response.text, "html.parser")
    #     articles = soup.select("article")
    #     if not articles:
    #         break  # No articles means end of pagination
    #     articles_page_data.append(articles)

    #     # Proceed to next page
    #     page += 1

